{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "y_test = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "n1 = 100\n",
    "no_users = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0], -1))/255\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], -1))/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters.\n",
    "n_hidden_1 = 128 # 1st layer number of neurons.\n",
    "n_hidden_2 = 64 # 2nd layer number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random.truncated_normal([784,n_hidden_1], stddev=0.1))\n",
    "W2 = tf.Variable(tf.random.truncated_normal([n_hidden_1,n_hidden_2], stddev=0.1))\n",
    "W3 = tf.Variable(tf.random.truncated_normal([n_hidden_2,10], stddev=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "    def __init__(self):       \n",
    "        self.GW1 = tf.Variable(tf.random.truncated_normal([784,n_hidden_1], stddev=0.1))\n",
    "        self.GW2 = tf.Variable(tf.random.truncated_normal([n_hidden_1,n_hidden_2], stddev=0.1))\n",
    "        self.GW3 = tf.Variable(tf.random.truncated_normal([n_hidden_2,10], stddev=0.1))\n",
    "        self.H = 1.0#np.random.normal()   \n",
    "        self.abs_H_square = self.H**2\n",
    "        \n",
    "\n",
    "    def neural_net(self, x):\n",
    "        y1 = tf.nn.relu(tf.matmul(x, W1))\n",
    "        y2 = tf.nn.relu(tf.matmul(y1, W2))\n",
    "        ylogits = tf.matmul(y2, W3)\n",
    "        return tf.nn.softmax(ylogits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-batch loss function.\n",
    "def mini_batches(X, Y, mb_size = 100):\n",
    "\n",
    "    m = X.shape[0]\n",
    "\n",
    "    perm = list(np.random.permutation(m))\n",
    "    #perm = perm_init[0:100]\n",
    "    X_temp = X[perm,:]\n",
    "    Y_temp = Y[perm,:].reshape((m, Y.shape[1]))\n",
    "    \n",
    "    X_r = tf.convert_to_tensor(X_temp[0:mb_size,:], dtype=np.float32)\n",
    "    Y_r = tf.convert_to_tensor(Y_temp[0:mb_size,:], dtype=np.float32)\n",
    "    return X_r,Y_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy loss function.\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    # Clip prediction values to avoid log(0) error.\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "    # Compute cross-entropy.\n",
    "    return -tf.reduce_sum(y_true * tf.math.log(y_pred))#tf.reduce_sum(tf.math.square(y_true-y_pred))#-tf.reduce_sum(y_true * tf.math.log(y_pred))\n",
    "    #tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n",
    "    #return tf.reduce_sum(-tf.reduce_mean(y_true * tf.math.log(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metric.\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [User() for i in range(no_users)]\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
    "central_modal = [tf.Variable(tf.random.truncated_normal([784*n_hidden_1,1], stddev=0.1)), tf.Variable(tf.random.truncated_normal([n_hidden_1*n_hidden_2,1], stddev=0.1)), tf.Variable(tf.random.truncated_normal([10*n_hidden_2,1], stddev=0.1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization process. \n",
    "def run_optimization(x, y, user, W1, W2, W3, rho):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    \n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch([W1,W2,W3])\n",
    "        pred = user.neural_net(x)\n",
    "        loss = cross_entropy(pred, y) \n",
    "        \n",
    "    \n",
    "    # Variables to update, i.e. trainable variables.\n",
    "    trainable_variables = [W1, W2 ,W3]\n",
    "    \n",
    "    # Compute gradients.\n",
    "    gradients1,gradients2,gradients3  = g.gradient(loss, trainable_variables)\n",
    "    #print(gradients2)\n",
    "    return gradients1,gradients2,gradients3, loss\n",
    "    #\n",
    "    \n",
    "    # Update W following gradients.\n",
    "    # optimizer.apply_gradients(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max(G):\n",
    "    \n",
    "    myMax = (tf.reduce_max(G)).numpy()\n",
    "    #myMax=R1_max.eval()\n",
    "    \n",
    "    myMin = (tf.reduce_min(G)).numpy()\n",
    "    #myMin=R1_min.eval()\n",
    "    \n",
    "    return myMax, myMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_k = []\n",
    "y_train_k = []\n",
    "data_per_worker = int(x_train.shape[0]/no_users)\n",
    "for i in range(no_users):\n",
    "    first = i*data_per_worker\n",
    "    last = first + data_per_worker\n",
    "    x_train_k.append(x_train[first:last])\n",
    "    y_train_k.append(y_train[first:last])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tf.convert_to_tensor(x_test, dtype=np.float32)\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[237.90766602]\n",
      "0.1158\n",
      "1\n",
      "[215.36623535]\n",
      "0.2827\n",
      "2\n",
      "[193.77445068]\n",
      "0.514\n",
      "3\n",
      "[186.42475586]\n",
      "0.3315\n",
      "4\n",
      "[283.83652344]\n",
      "0.29500002\n",
      "5\n",
      "[214.57287598]\n",
      "0.3076\n",
      "6\n",
      "[181.84833984]\n",
      "0.4216\n",
      "7\n",
      "[154.76889648]\n",
      "0.5091\n",
      "8\n",
      "[141.46520996]\n",
      "0.5612999\n",
      "9\n",
      "[157.66259766]\n",
      "0.47770005\n",
      "10\n",
      "[154.65856934]\n",
      "0.5197\n",
      "11\n",
      "[156.75202637]\n",
      "0.44349998\n",
      "12\n",
      "[134.00715332]\n",
      "0.54969996\n",
      "13\n",
      "[131.43372803]\n",
      "0.5512\n",
      "14\n",
      "[128.27862549]\n",
      "0.5773\n",
      "15\n",
      "[141.3057251]\n",
      "0.5143\n",
      "16\n",
      "[111.7651123]\n",
      "0.6408\n",
      "17\n",
      "[101.34848022]\n",
      "0.6889\n",
      "18\n",
      "[105.05865479]\n",
      "0.6492\n",
      "19\n",
      "[109.34909668]\n",
      "0.5867\n",
      "20\n",
      "[116.31396484]\n",
      "0.5691\n",
      "21\n",
      "[86.35406494]\n",
      "0.72389996\n",
      "22\n",
      "[64.74172363]\n",
      "0.79129994\n",
      "23\n",
      "[63.10664673]\n",
      "0.8002\n",
      "24\n",
      "[74.05291748]\n",
      "0.7336999\n",
      "25\n",
      "[112.69796143]\n",
      "0.6492\n",
      "26\n",
      "[100.12169189]\n",
      "0.6571001\n",
      "27\n",
      "[84.66764526]\n",
      "0.7311\n",
      "28\n",
      "[74.27041016]\n",
      "0.7515\n",
      "29\n",
      "[61.72365112]\n",
      "0.7789999\n",
      "30\n",
      "[48.45714111]\n",
      "0.8557\n",
      "31\n",
      "[50.60075684]\n",
      "0.8447\n",
      "32\n",
      "[54.41091309]\n",
      "0.7993001\n",
      "33\n",
      "[55.71505737]\n",
      "0.811\n",
      "34\n",
      "[63.95345459]\n",
      "0.7992\n",
      "35\n",
      "[53.19732666]\n",
      "0.8372\n",
      "36\n",
      "[51.81642456]\n",
      "0.8456\n",
      "37\n",
      "[62.39022827]\n",
      "0.82580006\n",
      "38\n",
      "[59.52015381]\n",
      "0.8031\n",
      "39\n",
      "[48.3506012]\n",
      "0.8577\n",
      "40\n",
      "[43.34254456]\n",
      "0.8619\n",
      "41\n",
      "[34.52171021]\n",
      "0.88629997\n",
      "42\n",
      "[37.12631531]\n",
      "0.8985001\n",
      "43\n",
      "[37.00857239]\n",
      "0.8971999\n",
      "44\n",
      "[34.32884521]\n",
      "0.90209997\n",
      "45\n",
      "[36.40099792]\n",
      "0.89460003\n",
      "46\n",
      "[43.09121399]\n",
      "0.8505\n",
      "47\n",
      "[55.0897522]\n",
      "0.8277\n",
      "48\n",
      "[56.84091797]\n",
      "0.794\n",
      "49\n",
      "[37.26689453]\n",
      "0.89589995\n",
      "50\n",
      "[34.08890991]\n",
      "0.8976\n",
      "51\n",
      "[33.48805847]\n",
      "0.9038\n",
      "52\n",
      "[33.75698547]\n",
      "0.90480006\n",
      "53\n",
      "[34.79640198]\n",
      "0.90200007\n",
      "54\n",
      "[40.73795166]\n",
      "0.8825\n",
      "55\n",
      "[36.47229309]\n",
      "0.8913\n",
      "56\n",
      "[36.90968628]\n",
      "0.8934\n",
      "57\n",
      "[34.25028992]\n",
      "0.8942\n",
      "58\n",
      "[35.55335388]\n",
      "0.8978999\n",
      "59\n",
      "[28.95549622]\n",
      "0.90410006\n",
      "60\n",
      "[27.7901947]\n",
      "0.9052\n",
      "61\n",
      "[27.15415344]\n",
      "0.91780007\n",
      "62\n",
      "[24.97836761]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-ec57822d90d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0msum_gradient_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_gradient_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0msum_gradient_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_gradient_2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0msum_gradient_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_gradient_3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGW3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1195\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    531\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m    532\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AddV2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         name, _ctx._post_execution_callbacks, x, y)\n\u001b[0m\u001b[1;32m    534\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "abs_weights_diff = []\n",
    "abs_biases_diff = []\n",
    "Train_Acc = []\n",
    "Test_Acc = []\n",
    "CrE_Train = []\n",
    "CrE_Test = []\n",
    "b=5\n",
    "tau=1/(2**b-1)\n",
    "lr = 10**(-2)\n",
    "\n",
    "mb_size = 100\n",
    "n_epochs = 500\n",
    "\n",
    "acc_train = np.zeros([n_epochs,1])\n",
    "acc_test = np.zeros([n_epochs,1])\n",
    "total_loss = np.zeros([n_epochs,1])\n",
    "H_matrix = np.zeros([int(n_epochs/10)])\n",
    "eps = 10**(-6)\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for k in range(n_epochs):\n",
    "        print(k)\n",
    "        \n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for i in range(no_users):\n",
    "            batch_xx , batch_yy = mini_batches(x_train_k[i],y_train_k[i],  mb_size)\n",
    "            batch_x.append(batch_xx)\n",
    "            batch_y.append(batch_yy)\n",
    "        \n",
    "        \n",
    "        W1 = tf.reshape(central_modal[0],[784, n_hidden_1])\n",
    "        W2 = tf.reshape(central_modal[1],[n_hidden_1, n_hidden_2])\n",
    "        W3 = tf.reshape(central_modal[2],[n_hidden_2, 10])\n",
    "        \n",
    "        # Run the optimization to update W  \n",
    "        for i in range(no_users):\n",
    "            gradients1, gradients2, gradients3, loss = run_optimization(batch_x[i], batch_y[i], users[i], W1, W2, W3, lr)\n",
    "            users[i].GW1 = gradients1\n",
    "            users[i].GW2 = gradients2\n",
    "            users[i].GW3 = gradients3\n",
    "            \n",
    "            G1 = tf.reshape(users[i].GW1,[784*n_hidden_1, 1])\n",
    "            G2 = tf.reshape(users[i].GW2,[n_hidden_1*n_hidden_2, 1])\n",
    "            G3 = tf.reshape(users[i].GW3,[10*n_hidden_2, 1])\n",
    "                        \n",
    "            [max_value, min_value]=find_max(G1)\n",
    "            R[0].assign(max(abs(min_value), abs(max_value)))\n",
    "            Q1=(G1+R[0])/(2*tau*R[0])+1/2\n",
    "            Q1=tf.math.floor(Q1)#.eval())\n",
    "            Q1= 2*tau*Q1*R[0]-R[0]\n",
    "            users[i].GW1=tf.reshape(Q1,[784,n_hidden_1])\n",
    "            \n",
    "            [max_value, min_value]=find_max(G2)\n",
    "            R[0].assign(max(abs(min_value), abs(max_value)))\n",
    "            Q2=(G2+R[0])/(2*tau*R[0])+1/2\n",
    "            Q2=tf.math.floor(Q2)#.eval())\n",
    "            Q2= 2*tau*Q2*R[0]-R[0]\n",
    "            users[i].GW2=tf.reshape(Q2,[n_hidden_1,n_hidden_2])\n",
    "            \n",
    "            [max_value, min_value]=find_max(G3)\n",
    "            R[0].assign(max(abs(min_value), abs(max_value)))\n",
    "            Q3=(G3+R[0])/(2*tau*R[0])+1/2\n",
    "            Q3=tf.math.floor(Q3)#.eval())\n",
    "            Q3= 2*tau*Q3*R[0]-R[0]\n",
    "            users[i].GW3=tf.reshape(Q3,[n_hidden_2*10])\n",
    "            \n",
    "            total_loss[k] = total_loss[k] + loss\n",
    "        \n",
    "        total_loss[k] = total_loss[k]/no_users\n",
    "        print(total_loss[k])\n",
    "            \n",
    "        sum_gradient_1 = 0\n",
    "        sum_gradient_2 = 0\n",
    "        sum_gradient_3 = 0\n",
    "        for i in range(no_users):\n",
    "            sum_gradient_1 = sum_gradient_1 + (users[i].GW1)\n",
    "            sum_gradient_2 = sum_gradient_2 + (users[i].GW2)\n",
    "            sum_gradient_3 = sum_gradient_3 + (users[i].GW3)\n",
    "                \n",
    "\n",
    "        sum_gradient_1 = tf.reshape(sum_gradient_1,[784*n_hidden_1, 1])\n",
    "        sum_gradient_2 = tf.reshape(sum_gradient_2,[n_hidden_1*n_hidden_2, 1])\n",
    "        sum_gradient_3 = tf.reshape(sum_gradient_3,[10*n_hidden_2, 1])\n",
    "        \n",
    "        central_modal[0] = central_modal[0] - lr*sum_gradient_1/no_users\n",
    "        central_modal[1] = central_modal[1] - lr*sum_gradient_2/no_users\n",
    "        central_modal[2] = central_modal[2] - lr*sum_gradient_3/no_users\n",
    "        \n",
    "        \n",
    "        train_acc = []\n",
    "        test_acc = []\n",
    "        for j in range(no_users):  \n",
    "            train_pred = users[j].neural_net(batch_x[j])\n",
    "            train_acc.append(accuracy(train_pred , batch_y[j]))\n",
    "            test_pred = users[j].neural_net(x_test)\n",
    "            test_acc.append(accuracy(test_pred , y_test))\n",
    "            \n",
    "        avgAcc_Train = np.mean(train_acc)\n",
    "        avgAcc_Test = np.mean(test_acc) \n",
    "        #print(avgAcc_Train)\n",
    "        print(avgAcc_Test)\n",
    "        acc_train[k] = avgAcc_Train\n",
    "        acc_test[k] = avgAcc_Test\n",
    "\n",
    "np.save('acc_test_qgd.npy', acc_test)\n",
    "#np.save('loss_gd_40960_5.npy', total_loss)\n",
    "#np.save('acc_train_gd_40960_5.npy', acc_train)\n",
    "#np.save('acc_test_gd_40960_5.npy', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('acc_test_qgd_5bits.npy', acc_test)\n",
    "#np.save('nb_ts_gd_lr001.npy', nb_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
